{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, clear_output\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate nonlinear regression data\n",
    "def generate_nonlinear_data(n_samples=100, noise=0.3, random_state=None):\n",
    "    \"\"\"Generate a nonlinear regression dataset.\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    X = np.random.uniform(-3, 3, n_samples).reshape(-1, 1)\n",
    "    y = np.sin(X.ravel()) + X.ravel()**2 / 6 + noise * np.random.randn(n_samples)\n",
    "    return X, y\n",
    "\n",
    "# Function to generate binary classification datasets\n",
    "def generate_binary_datasets(n_samples=200, noise=0.2, random_state=42):\n",
    "    \"\"\"Generate various binary classification datasets.\"\"\"\n",
    "    datasets = {\n",
    "        \"moons\": make_moons(n_samples=n_samples, noise=noise, random_state=random_state),\n",
    "        \"circles\": make_circles(n_samples=n_samples, noise=noise, factor=0.5, random_state=random_state),\n",
    "        \"linearly_separable\": make_classification(\n",
    "            n_samples=n_samples, n_features=2, n_redundant=0, n_informative=2,\n",
    "            random_state=random_state, n_clusters_per_class=1\n",
    "        )\n",
    "    }\n",
    "    return datasets\n",
    "\n",
    "# Function to generate multiclass data\n",
    "def generate_multiclass_data(n_samples=300, n_classes=3, n_features=2, random_state=42):\n",
    "    \"\"\"Generate a multiclass classification dataset.\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples, \n",
    "        n_features=n_features, \n",
    "        n_informative=n_features, \n",
    "        n_redundant=0,\n",
    "        n_classes=n_classes, \n",
    "        n_clusters_per_class=1, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "# Function to preprocess data (standardize and split)\n",
    "def preprocess_data(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"Standardize and split data into train and test sets.\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Kernel(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for kernels.\n",
    "    \n",
    "    A kernel function k(x, y) maps pairs of input points from the original space\n",
    "    to a similarity score in the feature space without explicitly computing the \n",
    "    coordinates in the feature space. This is known as the \"kernel trick\".\n",
    "    \n",
    "    In RKHS theory, a kernel function corresponds to an inner product in some \n",
    "    feature space: k(x, y) = <φ(x), φ(y)>, where φ is a feature map.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix between X1 and X2.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X1 : ndarray of shape (n_samples_1, n_features)\n",
    "            First set of samples\n",
    "        X2 : ndarray of shape (n_samples_2, n_features)\n",
    "            Second set of samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        K : ndarray of shape (n_samples_1, n_samples_2)\n",
    "            Kernel matrix\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compute_gram_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Compute the Gram matrix for a dataset X.\n",
    "        \n",
    "        The Gram matrix is a square matrix of kernel evaluations between all pairs\n",
    "        of points in the dataset. It is a key component in kernel methods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        K : ndarray of shape (n_samples, n_samples)\n",
    "            Gram matrix\n",
    "        \"\"\"\n",
    "        return self(X, X)\n",
    "    \n",
    "    def is_psd(self, X, tol=1e-8):\n",
    "        \"\"\"\n",
    "        Check if the kernel matrix is positive semi-definite (PSD).\n",
    "        \n",
    "        A valid kernel function must produce a PSD kernel matrix, which is a \n",
    "        fundamental property in RKHS theory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data\n",
    "        tol : float\n",
    "            Tolerance for eigenvalue positivity check\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        is_psd : bool\n",
    "            True if the kernel matrix is PSD, False otherwise\n",
    "        eigenvalues : ndarray\n",
    "            Eigenvalues of the kernel matrix\n",
    "        \"\"\"\n",
    "        K = self.compute_gram_matrix(X)\n",
    "        \n",
    "        # Ensure the matrix is symmetric\n",
    "        if not np.allclose(K, K.T):\n",
    "            print(\"Warning: Kernel matrix is not symmetric.\")\n",
    "            K = (K + K.T) / 2.0\n",
    "        \n",
    "        # Compute eigenvalues\n",
    "        eigenvalues = np.linalg.eigvalsh(K)\n",
    "        \n",
    "        # Check if all eigenvalues are positive (within numerical tolerance)\n",
    "        is_psd = np.all(eigenvalues > -tol)\n",
    "        \n",
    "        return is_psd, eigenvalues\n",
    "    \n",
    "    def get_feature_map_example(self, x):\n",
    "        \"\"\"\n",
    "        Return an example of the feature map for a single data point.\n",
    "        This is for educational purposes to show the explicit mapping.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray of shape (n_features,)\n",
    "            Input data point\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        phi_x : ndarray or str\n",
    "            Feature map of x or a description of the feature map\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Feature map example not implemented for this kernel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Linear kernel: k(x, y) = <x, y>\n",
    "    \n",
    "    The linear kernel is the simplest kernel function, corresponding to the \n",
    "    standard dot product in the input space. It doesn't map the data to a \n",
    "    higher-dimensional space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute the linear kernel matrix between X1 and X2.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X1 : ndarray of shape (n_samples_1, n_features)\n",
    "            First set of samples\n",
    "        X2 : ndarray of shape (n_samples_2, n_features)\n",
    "            Second set of samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        K : ndarray of shape (n_samples_1, n_samples_2)\n",
    "            Kernel matrix\n",
    "        \"\"\"\n",
    "        return np.dot(X1, X2.T)\n",
    "    \n",
    "    def get_feature_map_example(self, x):\n",
    "        \"\"\"\n",
    "        For the linear kernel, the feature map is the identity function.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray of shape (n_features,)\n",
    "            Input data point\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        phi_x : ndarray\n",
    "            Feature map of x (same as x for linear kernel)\n",
    "        \"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Polynomial kernel: k(x, y) = (gamma * <x, y> + coef0)^degree\n",
    "    \n",
    "    The polynomial kernel maps the data into a higher-dimensional space where\n",
    "    the new features correspond to all possible polynomial combinations of the\n",
    "    original features up to the specified degree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, degree=3, gamma=1.0, coef0=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the polynomial kernel.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        degree : int, default=3\n",
    "            Degree of the polynomial\n",
    "        gamma : float, default=1.0\n",
    "            Scale parameter\n",
    "        coef0 : float, default=1.0\n",
    "            Independent term\n",
    "        \"\"\"\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "    \n",
    "    def __call__(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute the polynomial kernel matrix between X1 and X2.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X1 : ndarray of shape (n_samples_1, n_features)\n",
    "            First set of samples\n",
    "        X2 : ndarray of shape (n_samples_2, n_features)\n",
    "            Second set of samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        K : ndarray of shape (n_samples_1, n_samples_2)\n",
    "            Kernel matrix\n",
    "        \"\"\"\n",
    "        return (self.gamma * np.dot(X1, X2.T) + self.coef0) ** self.degree\n",
    "    \n",
    "    def get_feature_map_example(self, x):\n",
    "        \"\"\"\n",
    "        Return an example of the feature map for the polynomial kernel.\n",
    "        This is only implemented for 2D data and degree 2 for simplicity.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray of shape (2,)\n",
    "            Input data point (must be 2D)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        phi_x : ndarray or str\n",
    "            Feature map of x or a description of the feature map\n",
    "        \"\"\"\n",
    "        if len(x) != 2 or self.degree != 2:\n",
    "            return (f\"For a polynomial kernel of degree {self.degree}, the feature map maps to a \"\n",
    "                   f\"space of dimension C(n+d,d) where n is the input dimension and d is the degree.\")\n",
    "        \n",
    "        # For a 2D input x = [x1, x2] and degree 2, the feature map is:\n",
    "        # φ(x) = [1, sqrt(2*gamma)*x1, sqrt(2*gamma)*x2, gamma*x1^2, sqrt(2)*gamma*x1*x2, gamma*x2^2]\n",
    "        # This is a simplified version assuming coef0 = 1\n",
    "        x1, x2 = x\n",
    "        sqrt_2gamma = np.sqrt(2 * self.gamma)\n",
    "        \n",
    "        return np.array([\n",
    "            1, \n",
    "            sqrt_2gamma * x1, \n",
    "            sqrt_2gamma * x2, \n",
    "            self.gamma * x1**2, \n",
    "            sqrt_2gamma * self.gamma * x1 * x2, \n",
    "            self.gamma * x2**2\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFKernel(Kernel):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (RBF) kernel: k(x, y) = exp(-gamma * ||x - y||^2)\n",
    "    \n",
    "    The RBF kernel, also known as the Gaussian kernel, maps the data into an \n",
    "    infinite-dimensional space. It is one of the most widely used kernels due to \n",
    "    its flexibility and theoretical properties. In RKHS theory, the RBF kernel \n",
    "    induces a space of smooth functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the RBF kernel.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        gamma : float, default=1.0\n",
    "            Scale parameter (inverse of the standard deviation)\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __call__(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute the RBF kernel matrix between X1 and X2.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X1 : ndarray of shape (n_samples_1, n_features)\n",
    "            First set of samples\n",
    "        X2 : ndarray of shape (n_samples_2, n_features)\n",
    "            Second set of samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        K : ndarray of shape (n_samples_1, n_samples_2)\n",
    "            Kernel matrix\n",
    "        \"\"\"\n",
    "        # Compute squared Euclidean distances efficiently\n",
    "        X1_norm = np.sum(X1 ** 2, axis=1).reshape(-1, 1)\n",
    "        X2_norm = np.sum(X2 ** 2, axis=1).reshape(1, -1)\n",
    "        distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)\n",
    "        \n",
    "        # Apply the RBF function\n",
    "        return np.exp(-self.gamma * distances)\n",
    "    \n",
    "    def get_feature_map_example(self, x):\n",
    "        \"\"\"\n",
    "        The RBF kernel maps to an infinite-dimensional space,\n",
    "        so we can't explicitly represent the full feature map.\n",
    "        However, we can show the first few terms of its Taylor expansion.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : ndarray of shape (n_features,)\n",
    "            Input data point\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        description : str\n",
    "            Description of the feature map\n",
    "        \"\"\"\n",
    "        return (\"The RBF kernel maps to an infinite-dimensional space. \"\n",
    "                \"Its feature map can be represented as an infinite series using \"\n",
    "                \"the Taylor expansion of the exponential function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_DICT = {\n",
    "    'linear': LinearKernel,\n",
    "    'polynomial': PolynomialKernel,\n",
    "    'rbf': RBFKernel,\n",
    "    \n",
    "}\n",
    "\n",
    "def get_kernel(kernel_name, **kernel_params):\n",
    "    \"\"\"\n",
    "    Get a kernel instance by name.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kernel_name : str\n",
    "        Name of the kernel. Must be one of: 'linear', 'polynomial', 'rbf', \n",
    "        'laplacian', 'sigmoid'.\n",
    "    **kernel_params : dict\n",
    "        Additional parameters to be passed to the kernel constructor.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    kernel : Kernel\n",
    "        Kernel instance\n",
    "    \"\"\"\n",
    "    if kernel_name not in KERNEL_DICT:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel_name}. Available kernels: {list(KERNEL_DICT.keys())}\")\n",
    "    \n",
    "    return KERNEL_DICT[kernel_name](**kernel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f36c16bb5e4e38a3a00a1beb0d5bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Dataset Type:', layout=Layout(width='auto'), options=(('Regression…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _visualize_kernel(dataset_type='regression', kernel_type='rbf', gamma=1.0, degree=2, coef0=1.0, elev=30, azim=30):\n",
    "    \"\"\"\n",
    "    Academic visualization of kernel properties focusing on dataset, kernel heatmap, and feature space mapping.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_type : str\n",
    "        Type of dataset to use ('regression', 'binary', 'multiclass')\n",
    "    kernel_type : str\n",
    "        Type of kernel to visualize ('linear', 'polynomial', 'rbf')\n",
    "    gamma : float\n",
    "        Gamma parameter for kernels\n",
    "    degree : int\n",
    "        Degree for polynomial kernel\n",
    "    coef0 : float\n",
    "        Coefficient for polynomial kernel\n",
    "    elev : int\n",
    "        Elevation angle for 3D plot\n",
    "    azim : int\n",
    "        Azimuth angle for 3D plot\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate appropriate dataset\n",
    "    if dataset_type == 'regression':\n",
    "        # Use a fixed grid for regression to avoid random patterns\n",
    "        X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "        y = np.sin(X.ravel()) + X.ravel()**2 / 6 + 0.3 * np.random.randn(100)\n",
    "        title_prefix = 'Regression'\n",
    "        X_vis = X\n",
    "    elif dataset_type == 'binary':\n",
    "        datasets = generate_binary_datasets(n_samples=200, noise=0.2, random_state=42)\n",
    "        X, y = datasets['moons']\n",
    "        title_prefix = 'Binary Classification (Moons)'\n",
    "        X_vis = X\n",
    "    elif dataset_type == 'multiclass':\n",
    "        X, y = generate_multiclass_data(n_samples=300, n_classes=3, n_features=2, random_state=42)\n",
    "        title_prefix = 'Multiclass Classification'\n",
    "        X_vis = X\n",
    "    \n",
    "    # Create figure with academic styling\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot original data\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    if dataset_type == 'regression':\n",
    "        scatter = ax1.scatter(X.ravel(), y, c=X.ravel(), cmap='viridis', s=40, alpha=0.8, edgecolors='w', linewidth=0.5)\n",
    "        ax1.set_title(f'{title_prefix} Dataset', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('$x$', fontsize=12)\n",
    "        ax1.set_ylabel('$y$', fontsize=12)\n",
    "        fig.colorbar(scatter, ax=ax1, label='$x$ value')\n",
    "    else:\n",
    "        scatter = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=40, alpha=0.8, edgecolors='w', linewidth=0.5)\n",
    "        ax1.set_title(f'{title_prefix} Dataset', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('$x_1$', fontsize=12)\n",
    "        ax1.set_ylabel('$x_2$', fontsize=12)\n",
    "        legend1 = ax1.legend(*scatter.legend_elements(), title=\"Classes\", frameon=True)\n",
    "        legend1.get_frame().set_facecolor('white')\n",
    "        legend1.get_frame().set_alpha(0.9)\n",
    "        ax1.add_artist(legend1)\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Get kernel\n",
    "    if kernel_type == 'linear':\n",
    "        kernel = LinearKernel()\n",
    "        kernel_title = 'Linear Kernel'\n",
    "        kernel_formula = r'$k(x, y) = x^T y$'\n",
    "    elif kernel_type == 'polynomial':\n",
    "        kernel = PolynomialKernel(degree=degree, gamma=gamma, coef0=coef0)\n",
    "        kernel_title = f'Polynomial Kernel (degree={degree})'\n",
    "        kernel_formula = r'$k(x, y) = (\\gamma x^T y + c_0)^{' + str(degree) + r'}$'\n",
    "    elif kernel_type == 'rbf':\n",
    "        kernel = RBFKernel(gamma=gamma)\n",
    "        kernel_title = f'RBF Kernel'\n",
    "        kernel_formula = r'$k(x, y) = \\exp(-\\gamma ||x - y||^2)$'\n",
    "    \n",
    "    # Compute kernel heatmap\n",
    "    if dataset_type == 'regression':\n",
    "        # For regression, create a 2D grid to visualize the kernel function\n",
    "        x_min, x_max = -3, 3\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50), np.linspace(x_min, x_max, 50))\n",
    "        grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "        \n",
    "        # Compute kernel values for a fixed point against all grid points\n",
    "        fixed_point = np.array([0.0]).reshape(1, -1)  # Center point\n",
    "        if dataset_type == 'regression':\n",
    "            K_values = kernel(fixed_point, X_vis)\n",
    "        else:\n",
    "            K_values = kernel(fixed_point, grid_points.reshape(-1, 2))\n",
    "        \n",
    "        # Plot kernel heatmap\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        \n",
    "        if dataset_type == 'regression':\n",
    "            # For 1D data, plot kernel values against input points\n",
    "            ax2.scatter(X_vis.ravel(), K_values.ravel(), c=X_vis.ravel(), cmap='viridis', \n",
    "                       s=40, alpha=0.8, edgecolors='w', linewidth=0.5)\n",
    "            ax2.set_title(f'{kernel_title} Values\\n{kernel_formula}', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('$x$', fontsize=12)\n",
    "            ax2.set_ylabel('$k(0, x)$', fontsize=12)\n",
    "            ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        else:\n",
    "            # For 2D data, create a heatmap\n",
    "            K_grid = K_values.reshape(xx.shape)\n",
    "            im = ax2.contourf(xx, yy, K_grid, levels=50, cmap='viridis', alpha=0.8)\n",
    "            ax2.scatter(fixed_point[0, 0], fixed_point[0, 0], c='red', s=100, marker='*', \n",
    "                       edgecolors='w', linewidth=1.5, label='Reference Point')\n",
    "            ax2.set_title(f'{kernel_title} Values\\n{kernel_formula}', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('$x_1$', fontsize=12)\n",
    "            ax2.set_ylabel('$x_2$', fontsize=12)\n",
    "            ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax2.legend(frameon=True)\n",
    "            fig.colorbar(im, ax=ax2, label='$k(0, x)$')\n",
    "    else:\n",
    "        # For classification datasets, visualize decision boundaries\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50), np.linspace(y_min, y_max, 50))\n",
    "        grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "        \n",
    "        # Compute kernel values for a fixed point against all grid points\n",
    "        fixed_point = np.array([0.0, 0.0]).reshape(1, -1)  # Center point\n",
    "        K_values = kernel(fixed_point, grid_points)\n",
    "        K_grid = K_values.reshape(xx.shape)\n",
    "        \n",
    "        # Plot kernel heatmap\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        im = ax2.contourf(xx, yy, K_grid, levels=50, cmap='viridis', alpha=0.8)\n",
    "        ax2.scatter(fixed_point[0, 0], fixed_point[0, 1], c='red', s=100, marker='*', \n",
    "                   edgecolors='w', linewidth=1.5, label='Reference Point')\n",
    "        ax2.set_title(f'{kernel_title} Values\\n{kernel_formula}', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('$x_1$', fontsize=12)\n",
    "        ax2.set_ylabel('$x_2$', fontsize=12)\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax2.legend(frameon=True)\n",
    "        fig.colorbar(im, ax=ax2, label='$k(0, x)$')\n",
    "    \n",
    "    # Feature space mapping visualization\n",
    "    ax3 = fig.add_subplot(133, projection='3d')\n",
    "    \n",
    "    # Create feature mapping based on the kernel\n",
    "    if dataset_type == 'regression':\n",
    "        # For 1D regression data\n",
    "        if kernel_type == 'linear':\n",
    "            # For linear kernel, map to 3D for visualization\n",
    "            X_mapped = np.column_stack([\n",
    "                X.ravel(),\n",
    "                y,\n",
    "                np.ones_like(X.ravel()) * 0.1  # Constant third dimension\n",
    "            ])\n",
    "            feature_map_title = 'Linear Kernel Feature Map\\n$\\\\phi(x) = x$'\n",
    "        elif kernel_type == 'polynomial':\n",
    "            # For polynomial kernel, show polynomial terms\n",
    "            if degree == 2:\n",
    "                X_mapped = np.column_stack([\n",
    "                    X.ravel(),\n",
    "                    y,\n",
    "                    np.sqrt(gamma) * X.ravel()**2  # Quadratic term\n",
    "                ])\n",
    "                feature_map_title = f'Polynomial Kernel Feature Map\\n$\\\\phi(x) = [x, x^2]$'\n",
    "            else:\n",
    "                X_mapped = np.column_stack([\n",
    "                    X.ravel(),\n",
    "                    y,\n",
    "                    np.sqrt(gamma) * X.ravel()**degree  # Higher degree term\n",
    "                ])\n",
    "                feature_map_title = f'Polynomial Kernel Feature Map\\n$\\\\phi(x) = [x, x^{degree}]$'\n",
    "        elif kernel_type == 'rbf':\n",
    "            # For RBF kernel, show radial basis functions\n",
    "            radial = np.exp(-gamma * X.ravel()**2)\n",
    "            X_mapped = np.column_stack([\n",
    "                X.ravel(),\n",
    "                y,\n",
    "                radial\n",
    "            ])\n",
    "            feature_map_title = f'RBF Kernel Feature Map\\n$\\\\phi(x) = [\\\\exp(-\\\\gamma ||x||^2)]$'\n",
    "    else:\n",
    "        # For 2D classification data\n",
    "        if kernel_type == 'linear':\n",
    "            # For linear kernel, add a constant third dimension\n",
    "            X_mapped = np.column_stack([\n",
    "                X[:, 0],\n",
    "                X[:, 1],\n",
    "                np.ones(X.shape[0]) * 0.1\n",
    "            ])\n",
    "            feature_map_title = 'Linear Kernel Feature Map\\n$\\\\phi(x) = x$'\n",
    "        elif kernel_type == 'polynomial':\n",
    "            # For polynomial kernel with 2D data\n",
    "            if degree == 2:\n",
    "                # For degree 2, show a simplified feature map\n",
    "                X_mapped = np.column_stack([\n",
    "                    X[:, 0],\n",
    "                    X[:, 1],\n",
    "                    np.sqrt(gamma) * (X[:, 0]**2 + X[:, 1]**2)  # Simplified quadratic term\n",
    "                ])\n",
    "                feature_map_title = f'Polynomial Kernel Feature Map\\n$\\\\phi(x) = [x_1, x_2, x_1^2+x_2^2]$'\n",
    "            else:\n",
    "                X_mapped = np.column_stack([\n",
    "                    X[:, 0],\n",
    "                    X[:, 1],\n",
    "                    np.sqrt(gamma) * (X[:, 0]**degree + X[:, 1]**degree)  # Simplified higher degree term\n",
    "                ])\n",
    "                feature_map_title = f'Polynomial Kernel Feature Map\\n$\\\\phi(x) = [x_1, x_2, x_1^{degree}+x_2^{degree}]$'\n",
    "        elif kernel_type == 'rbf':\n",
    "            # For RBF kernel, show distance from origin\n",
    "            radial = np.exp(-gamma * np.sum(X**2, axis=1))\n",
    "            X_mapped = np.column_stack([\n",
    "                X[:, 0],\n",
    "                X[:, 1],\n",
    "                radial\n",
    "            ])\n",
    "            feature_map_title = f'RBF Kernel Feature Map\\n$\\\\phi(x) = [\\\\exp(-\\\\gamma ||x||^2)]$'\n",
    "    \n",
    "    # Plot the mapped data\n",
    "    if dataset_type == 'regression':\n",
    "        scatter = ax3.scatter(X_mapped[:, 0], X_mapped[:, 1], X_mapped[:, 2], \n",
    "                             c=X.ravel(), cmap='viridis', s=40, alpha=0.8, edgecolors='w', linewidth=0.5)\n",
    "    else:\n",
    "        scatter = ax3.scatter(X_mapped[:, 0], X_mapped[:, 1], X_mapped[:, 2], \n",
    "                             c=y, cmap='viridis', s=40, alpha=0.8, edgecolors='w', linewidth=0.5)\n",
    "    \n",
    "    ax3.set_title(feature_map_title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if dataset_type == 'regression':\n",
    "        ax3.set_xlabel('$x$', fontsize=12)\n",
    "        ax3.set_ylabel('$y$', fontsize=12)\n",
    "    else:\n",
    "        ax3.set_xlabel('$x_1$', fontsize=12)\n",
    "        ax3.set_ylabel('$x_2$', fontsize=12)\n",
    "    \n",
    "    ax3.set_zlabel('$\\\\phi(x)_3$', fontsize=12)\n",
    "    \n",
    "    # Set the viewing angle\n",
    "    ax3.view_init(elev=elev, azim=azim)\n",
    "    \n",
    "    # Add grid to 3D plot\n",
    "    ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add parameter information\n",
    "    if kernel_type == 'polynomial':\n",
    "        param_text = f'Parameters: $\\\\gamma={gamma:.2f}$, $c_0={coef0:.2f}$, $d={degree}$'\n",
    "    elif kernel_type == 'rbf':\n",
    "        param_text = f'Parameter: $\\\\gamma={gamma:.2f}$'\n",
    "    else:\n",
    "        param_text = ''\n",
    "    \n",
    "    if param_text:\n",
    "        fig.text(0.5, 0.01, param_text, ha='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive widget with academic styling\n",
    "def interactive_kernel():\n",
    "    style_widget = {'description_width': 'initial'}\n",
    "    layout_widget = {'width': 'auto'}\n",
    "    \n",
    "    interact(\n",
    "        _visualize_kernel,\n",
    "        dataset_type=widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Regression', 'regression'), \n",
    "                ('Binary Classification', 'binary'), \n",
    "                ('Multiclass Classification', 'multiclass')\n",
    "            ],\n",
    "            value='regression',\n",
    "            description='Dataset Type:',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        ),\n",
    "        kernel_type=widgets.Dropdown(\n",
    "            options=[\n",
    "                ('Linear Kernel', 'linear'), \n",
    "                ('Polynomial Kernel', 'polynomial'), \n",
    "                ('RBF Kernel', 'rbf')\n",
    "            ],\n",
    "            value='rbf',\n",
    "            description='Kernel Function:',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        ),\n",
    "        gamma=widgets.FloatLogSlider(\n",
    "            value=1.0,\n",
    "            base=10,\n",
    "            min=-1,  # 10^-1\n",
    "            max=1,   # 10^1\n",
    "            step=0.1,\n",
    "            description='γ (Scale Parameter):',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        ),\n",
    "        degree=widgets.IntSlider(\n",
    "            value=2,\n",
    "            min=2,\n",
    "            max=5,\n",
    "            step=1,\n",
    "            description='Degree (Polynomial):',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        ),\n",
    "        coef0=widgets.FloatSlider(\n",
    "            value=1.0,\n",
    "            min=0.0,\n",
    "            max=2.0,\n",
    "            step=0.1,\n",
    "            description='c₀ (Polynomial):',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        ),\n",
    "        elev=widgets.IntSlider(\n",
    "            value=30,\n",
    "            min=0,\n",
    "            max=90,\n",
    "            step=5,\n",
    "            description='Elevation (3D View):',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        ),\n",
    "        azim=widgets.IntSlider(\n",
    "            value=30,\n",
    "            min=-180,\n",
    "            max=180,\n",
    "            step=5,\n",
    "            description='Azimuth (3D View):',\n",
    "            style=style_widget,\n",
    "            layout=layout_widget\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Run the interactive visualization\n",
    "interactive_kernel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KernelRidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Kernel Ridge Regression.\n",
    "    \n",
    "    This class implements Kernel Ridge Regression, which combines Ridge Regression\n",
    "    with the kernel trick to learn non-linear functions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kernel : str or callable, default='linear'\n",
    "        Kernel name or kernel function. If string, must be one of the kernels\n",
    "        available in kernels.py.\n",
    "    lambda_reg : float, default=1.0\n",
    "        Regularization parameter.\n",
    "    kernel_params : dict, default=None\n",
    "        Additional parameters for the kernel function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel='linear', lambda_reg=1.0, kernel_params=None):\n",
    "        self.kernel = kernel\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.kernel_params = kernel_params or {}\n",
    "        self.dual_coef_ = None\n",
    "        self.X_train_ = None\n",
    "        self._kernel_fn = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Kernel Ridge Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : numpy.ndarray, shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Store training data for prediction\n",
    "        self.X_train_ = X\n",
    "        \n",
    "        # Get kernel function\n",
    "        if isinstance(self.kernel, str):\n",
    "            self._kernel_fn = get_kernel(self.kernel, **self.kernel_params)\n",
    "        else:\n",
    "            self._kernel_fn = self.kernel\n",
    "        \n",
    "        # Compute Gram matrix\n",
    "        K = self._kernel_fn.compute_gram_matrix(X)\n",
    "        \n",
    "        # Add regularization to diagonal\n",
    "        K_reg = K + self.lambda_reg * np.eye(K.shape[0])\n",
    "        \n",
    "        # Solve the dual problem\n",
    "        self.dual_coef_ = np.linalg.solve(K_reg, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the Kernel Ridge Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : numpy.ndarray, shape (n_samples,)\n",
    "            Predicted values.\n",
    "        \"\"\"\n",
    "        # Compute kernel between test points and training points\n",
    "        K_test = self._kernel_fn(X, self.X_train_)\n",
    "        \n",
    "        # Compute predictions\n",
    "        y_pred = np.dot(K_test, self.dual_coef_)\n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge Regression Comparison\n",
      "=================================\n",
      "Select a kernel type and parameters to see performance metrics and visualization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8490938e7e403bbd2b90a20f825350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Kernel Type:', index=2, layout=Layout(width='auto'), options=(('Li…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5708e6516874479cbcfea3546171d6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_kernel_regression_tabular(kernel_type='rbf', gamma=1.0, degree=3):\n",
    "    \"\"\"\n",
    "    Evaluate Kernel Ridge Regression with the specified kernel type and parameters.\n",
    "    Displays results in a clean tabular format using tabulate and visualizes feature space.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kernel_type : str\n",
    "        Type of kernel ('linear', 'polynomial', or 'rbf')\n",
    "    gamma : float\n",
    "        Gamma parameter for RBF and polynomial kernels\n",
    "    degree : int\n",
    "        Degree parameter for polynomial kernel\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : KernelRidgeRegression\n",
    "        The best model found\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    \n",
    "    # Generate nonlinear regression data\n",
    "    print(\"Generating regression dataset...\")\n",
    "    X = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "    y = np.sin(X.ravel()) + X.ravel()**2 / 6 + 0.3 * np.random.randn(200)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define regularization parameters to evaluate\n",
    "    lambda_regs = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "    \n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_score = float('-inf')  # For R², higher is better\n",
    "    \n",
    "    # Set kernel parameters based on kernel type\n",
    "    if kernel_type == 'linear':\n",
    "        kernel_params = {}\n",
    "        print(f\"Evaluating Linear Kernel with different regularization values...\")\n",
    "    elif kernel_type == 'polynomial':\n",
    "        kernel_params = {'degree': degree, 'gamma': gamma, 'coef0': 1.0}\n",
    "        print(f\"Evaluating Polynomial Kernel (degree={degree}, gamma={gamma}) with different regularization values...\")\n",
    "    elif kernel_type == 'rbf':\n",
    "        kernel_params = {'gamma': gamma}\n",
    "        print(f\"Evaluating RBF Kernel (gamma={gamma}) with different regularization values...\")\n",
    "    \n",
    "    # Evaluate models with different regularization parameters\n",
    "    for lambda_reg in lambda_regs:\n",
    "        model = KernelRidgeRegression(kernel=kernel_type, lambda_reg=lambda_reg, kernel_params=kernel_params)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict on training and test sets\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate R² scores\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'λ': lambda_reg,\n",
    "            'Train R²': train_r2,\n",
    "            'Test R²': test_r2\n",
    "        })\n",
    "        \n",
    "        # Track best model (highest test R²)\n",
    "        if test_r2 > best_score:\n",
    "            best_score = test_r2\n",
    "            best_model = model\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display results in a nicely formatted table using tabulate\n",
    "    print(\"\\nPerformance Metrics (R² Score):\")\n",
    "    print(tabulate(results_df, headers='keys', tablefmt='fancy_grid', \n",
    "                  floatfmt='.6f', showindex=False, numalign='center'))\n",
    "    \n",
    "    # Highlight the best model\n",
    "    best_lambda = results_df.loc[results_df['Test R²'].idxmax()]['λ']\n",
    "    print(f\"\\nBest Model: {kernel_type.capitalize()} Kernel with λ={best_lambda}\")\n",
    "    print(f\"Best Test R²: {best_score:.6f}\")\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Visualize the model predictions\n",
    "    visualize_model(X, y, best_model, scaler, kernel_type, best_lambda, gamma, degree, ax=ax1)\n",
    "    \n",
    "    # Visualize feature space\n",
    "    visualize_feature_space(X_train_scaled, y_train, kernel_type, kernel_params, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def visualize_model(X, y, model, scaler, kernel_type, lambda_reg, gamma=None, degree=None, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize the model predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input features\n",
    "    y : numpy.ndarray\n",
    "        Target values\n",
    "    model : KernelRidgeRegression\n",
    "        Trained model\n",
    "    scaler : StandardScaler\n",
    "        Scaler used to standardize features\n",
    "    kernel_type : str\n",
    "        Type of kernel\n",
    "    lambda_reg : float\n",
    "        Regularization parameter\n",
    "    gamma : float\n",
    "        Gamma parameter (for RBF and polynomial kernels)\n",
    "    degree : int\n",
    "        Degree parameter (for polynomial kernel)\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Axes to plot on\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot original data\n",
    "    ax.scatter(X, y, c='black', s=30, alpha=0.5, label='Data')\n",
    "    \n",
    "    # Create a grid for smooth curves\n",
    "    X_plot = np.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n",
    "    X_plot_scaled = scaler.transform(X_plot)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_plot_scaled)\n",
    "    \n",
    "    # Plot prediction\n",
    "    if kernel_type == 'linear':\n",
    "        ax.plot(X_plot, y_pred, color='blue', linewidth=3, \n",
    "                label=f\"Linear Kernel (λ={lambda_reg:.3f})\")\n",
    "    elif kernel_type == 'polynomial':\n",
    "        ax.plot(X_plot, y_pred, color='red', linewidth=3, \n",
    "                label=f\"Polynomial Kernel (d={degree}, γ={gamma:.3f}, λ={lambda_reg:.3f})\")\n",
    "    elif kernel_type == 'rbf':\n",
    "        ax.plot(X_plot, y_pred, color='green', linewidth=3, \n",
    "                label=f\"RBF Kernel (γ={gamma:.3f}, λ={lambda_reg:.3f})\")\n",
    "    \n",
    "    ax.set_title(\"Kernel Ridge Regression\", fontsize=16)\n",
    "    ax.set_xlabel(\"x\", fontsize=14)\n",
    "    ax.set_ylabel(\"y\", fontsize=14)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "def visualize_feature_space(X, y, kernel_type, kernel_params, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize the feature space induced by the kernel.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Input features (scaled)\n",
    "    y : numpy.ndarray\n",
    "        Target values\n",
    "    kernel_type : str\n",
    "        Type of kernel\n",
    "    kernel_params : dict\n",
    "        Kernel parameters\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Axes to plot on\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA, KernelPCA\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Get kernel instance\n",
    "    kernel = get_kernel(kernel_type, **kernel_params)\n",
    "    \n",
    "    # Compute Gram matrix\n",
    "    K = kernel.compute_gram_matrix(X)\n",
    "    \n",
    "    # Use a subset of points for visualization (max 100)\n",
    "    n_samples = min(100, X.shape[0])\n",
    "    indices = np.linspace(0, X.shape[0]-1, n_samples).astype(int)\n",
    "    K_subset = K[indices][:, indices]\n",
    "    y_subset = y[indices]\n",
    "    \n",
    "    # Check if kernel matrix is PSD\n",
    "    is_psd, eigenvalues = kernel.is_psd(X[indices])\n",
    "    \n",
    "    # For 1D data, we need a special approach\n",
    "    if X.shape[1] == 1:\n",
    "        # For 1D data, we'll plot the original feature vs the target\n",
    "        if kernel_type == 'linear':\n",
    "            title = \"Linear Feature Space (Original Space)\"\n",
    "            # Just plot X vs y for 1D data\n",
    "            X_pca = np.column_stack([X[indices].flatten(), np.zeros(len(indices))])\n",
    "        else:\n",
    "            # For non-linear kernels with 1D data, use the kernel matrix itself\n",
    "            # Use MDS on the kernel matrix to get a 2D representation\n",
    "            from sklearn.manifold import MDS\n",
    "            mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "            # Convert kernel matrix to distance matrix (higher similarity = lower distance)\n",
    "            D = 1 - K_subset / np.max(K_subset)\n",
    "            X_pca = mds.fit_transform(D)\n",
    "            \n",
    "            if kernel_type == 'polynomial':\n",
    "                title = f\"Polynomial Kernel Feature Space (d={kernel_params['degree']})\"\n",
    "            elif kernel_type == 'rbf':\n",
    "                title = f\"RBF Kernel Feature Space (γ={kernel_params['gamma']:.3f})\"\n",
    "    else:\n",
    "        # Normal case for data with at least 2 dimensions\n",
    "        if kernel_type == 'linear':\n",
    "            # For linear kernel, we can just use PCA on the original data\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X[indices])\n",
    "            title = \"Linear Feature Space (Original Space)\"\n",
    "        else:\n",
    "            # For non-linear kernels, use Kernel PCA\n",
    "            if kernel_type == 'polynomial':\n",
    "                kpca = KernelPCA(n_components=2, kernel='poly', \n",
    "                                gamma=kernel_params.get('gamma', 1.0),\n",
    "                                degree=kernel_params.get('degree', 3),\n",
    "                                coef0=kernel_params.get('coef0', 1.0))\n",
    "                title = f\"Polynomial Kernel Feature Space (d={kernel_params['degree']})\"\n",
    "            elif kernel_type == 'rbf':\n",
    "                kpca = KernelPCA(n_components=2, kernel='rbf', \n",
    "                                gamma=kernel_params.get('gamma', 1.0))\n",
    "                title = f\"RBF Kernel Feature Space (γ={kernel_params['gamma']:.3f})\"\n",
    "            \n",
    "            X_pca = kpca.fit_transform(X[indices])\n",
    "    \n",
    "    # Plot the points in the feature space\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y_subset, cmap='viridis', \n",
    "                        s=50, alpha=0.8, edgecolors='k')\n",
    "    \n",
    "    # Add a colorbar\n",
    "    plt.colorbar(scatter, ax=ax, label='Target Value')\n",
    "    \n",
    "    # Add eigenvalue information\n",
    "    if is_psd:\n",
    "        psd_status = \"PSD ✓\"\n",
    "    else:\n",
    "        psd_status = \"Not PSD ✗\"\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title(f\"{title}\\n({psd_status}, λ_max={eigenvalues.max():.2f}, λ_min={eigenvalues.min():.2f})\", \n",
    "                fontsize=16)\n",
    "    ax.set_xlabel(\"Principal Component 1\", fontsize=14)\n",
    "    ax.set_ylabel(\"Principal Component 2\", fontsize=14)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "def compare_kernels_interactive():\n",
    "    \"\"\"\n",
    "    Interactive comparison of different kernel methods.\n",
    "    \"\"\"\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Create output widget to control display\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Define the update function\n",
    "    def update(kernel_type, gamma, degree):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            evaluate_kernel_regression_tabular(kernel_type, gamma, degree)\n",
    "    \n",
    "    # Create interactive widgets\n",
    "    style_widget = {'description_width': 'initial'}\n",
    "    layout_widget = {'width': 'auto'}\n",
    "    \n",
    "    kernel_widget = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('Linear Kernel', 'linear'), \n",
    "            ('Polynomial Kernel', 'polynomial'), \n",
    "            ('RBF Kernel', 'rbf')\n",
    "        ],\n",
    "        value='rbf',\n",
    "        description='Kernel Type:',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    gamma_widget = widgets.FloatLogSlider(\n",
    "        value=1.0,\n",
    "        base=10,\n",
    "        min=-2,  # 10^-2\n",
    "        max=1,   # 10^1\n",
    "        step=0.1,\n",
    "        description='γ (Scale Parameter):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    degree_widget = widgets.IntSlider(\n",
    "        value=3,\n",
    "        min=2,\n",
    "        max=5,\n",
    "        step=1,\n",
    "        description='Degree (Polynomial):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    # Create interactive widget\n",
    "    interactive_widget = widgets.interactive(\n",
    "        update,\n",
    "        kernel_type=kernel_widget,\n",
    "        gamma=gamma_widget,\n",
    "        degree=degree_widget\n",
    "    )\n",
    "    \n",
    "    # Display the widgets and output\n",
    "    display(interactive_widget)\n",
    "    display(output)\n",
    "    \n",
    "    # Initial update\n",
    "    update(kernel_widget.value, gamma_widget.value, degree_widget.value)\n",
    "    \n",
    "    \n",
    "print(\"Kernel Ridge Regression Comparison\")\n",
    "print(\"=================================\")\n",
    "print(\"Select a kernel type and parameters to see performance metrics and visualization.\")\n",
    "compare_kernels_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KernelSVM:\n",
    "    \"\"\"\n",
    "    Implementation of Kernel SVM using the dual formulation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kernel : str, default='rbf'\n",
    "        Kernel type. Can be 'linear', 'polynomial', or 'rbf'.\n",
    "    C : float, default=1.0\n",
    "        Regularization parameter. The strength of the regularization is\n",
    "        inversely proportional to C.\n",
    "    gamma : float, default=1.0\n",
    "        Kernel coefficient for 'rbf' and 'polynomial' kernels.\n",
    "    degree : int, default=3\n",
    "        Degree of the polynomial kernel.\n",
    "    coef0 : float, default=1.0\n",
    "        Independent term in the polynomial kernel.\n",
    "    tol : float, default=1e-3\n",
    "        Tolerance for stopping criterion.\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations for the solver.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel='rbf', C=1.0, gamma=1.0, degree=3, coef0=1.0, tol=1e-3, max_iter=1000):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.support_vectors_ = None\n",
    "        self.support_vector_indices_ = None\n",
    "        self.dual_coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.classes_ = None\n",
    "        self.X_train_ = None\n",
    "        \n",
    "    def _kernel_function(self, X1, X2):\n",
    "        \"\"\"Compute the kernel matrix between X1 and X2.\"\"\"\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(X1, X2.T)\n",
    "        elif self.kernel == 'polynomial':\n",
    "            return (self.gamma * np.dot(X1, X2.T) + self.coef0) ** self.degree\n",
    "        elif self.kernel == 'rbf':\n",
    "            # Compute squared Euclidean distances efficiently\n",
    "            X1_norm = np.sum(X1 ** 2, axis=1).reshape(-1, 1)\n",
    "            X2_norm = np.sum(X2 ** 2, axis=1).reshape(1, -1)\n",
    "            distances = X1_norm + X2_norm - 2 * np.dot(X1, X2.T)\n",
    "            return np.exp(-self.gamma * distances)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SVM model according to the given training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training vectors.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values. Should contain only two classes.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # Store training data\n",
    "        self.X_train_ = X\n",
    "        \n",
    "        # Convert y to {-1, 1}\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) != 2:\n",
    "            raise ValueError(\"KernelSVM only supports binary classification\")\n",
    "        \n",
    "        y_binary = np.where(y == self.classes_[0], -1, 1)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute the kernel matrix\n",
    "        K = self._kernel_function(X, X)\n",
    "        \n",
    "        # Solve the dual problem using SMO algorithm (simplified)\n",
    "        # Initialize alphas and bias\n",
    "        alphas = np.zeros(n_samples)\n",
    "        b = 0\n",
    "        \n",
    "        # SMO algorithm\n",
    "        for _ in range(self.max_iter):\n",
    "            alpha_changed = 0\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Calculate Ei = f(xi) - yi\n",
    "                f_i = np.sum(alphas * y_binary * K[i]) + b\n",
    "                E_i = f_i - y_binary[i]\n",
    "                \n",
    "                # Check if alpha_i can be optimized\n",
    "                if (y_binary[i] * E_i < -self.tol and alphas[i] < self.C) or \\\n",
    "                   (y_binary[i] * E_i > self.tol and alphas[i] > 0):\n",
    "                    \n",
    "                    # Select j randomly\n",
    "                    j = np.random.choice([idx for idx in range(n_samples) if idx != i])\n",
    "                    \n",
    "                    # Calculate Ej\n",
    "                    f_j = np.sum(alphas * y_binary * K[j]) + b\n",
    "                    E_j = f_j - y_binary[j]\n",
    "                    \n",
    "                    # Save old alphas\n",
    "                    alpha_i_old = alphas[i]\n",
    "                    alpha_j_old = alphas[j]\n",
    "                    \n",
    "                    # Compute bounds for alpha_j\n",
    "                    if y_binary[i] != y_binary[j]:\n",
    "                        L = max(0, alphas[j] - alphas[i])\n",
    "                        H = min(self.C, self.C + alphas[j] - alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, alphas[i] + alphas[j] - self.C)\n",
    "                        H = min(self.C, alphas[i] + alphas[j])\n",
    "                    \n",
    "                    if L == H:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute eta\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update alpha_j\n",
    "                    alphas[j] = alpha_j_old - y_binary[j] * (E_i - E_j) / eta\n",
    "                    \n",
    "                    # Clip alpha_j\n",
    "                    alphas[j] = min(H, max(L, alphas[j]))\n",
    "                    \n",
    "                    if abs(alphas[j] - alpha_j_old) < 1e-5:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update alpha_i\n",
    "                    alphas[i] = alpha_i_old + y_binary[i] * y_binary[j] * (alpha_j_old - alphas[j])\n",
    "                    \n",
    "                    # Update b\n",
    "                    b1 = b - E_i - y_binary[i] * (alphas[i] - alpha_i_old) * K[i, i] - \\\n",
    "                         y_binary[j] * (alphas[j] - alpha_j_old) * K[i, j]\n",
    "                    b2 = b - E_j - y_binary[i] * (alphas[i] - alpha_i_old) * K[i, j] - \\\n",
    "                         y_binary[j] * (alphas[j] - alpha_j_old) * K[j, j]\n",
    "                    \n",
    "                    if 0 < alphas[i] < self.C:\n",
    "                        b = b1\n",
    "                    elif 0 < alphas[j] < self.C:\n",
    "                        b = b2\n",
    "                    else:\n",
    "                        b = (b1 + b2) / 2\n",
    "                    \n",
    "                    alpha_changed += 1\n",
    "            \n",
    "            if alpha_changed == 0:\n",
    "                break\n",
    "        \n",
    "        # Save support vectors\n",
    "        sv_indices = alphas > 1e-5\n",
    "        self.support_vector_indices_ = np.where(sv_indices)[0]\n",
    "        self.support_vectors_ = X[sv_indices]\n",
    "        self.dual_coef_ = alphas[sv_indices] * y_binary[sv_indices]\n",
    "        self.intercept_ = b\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the decision function of X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        decision : array-like of shape (n_samples,)\n",
    "            Decision function values.\n",
    "        \"\"\"\n",
    "        K = self._kernel_function(X, self.support_vectors_)\n",
    "        return np.dot(K, self.dual_coef_) + self.intercept_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on samples in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array-like of shape (n_samples,)\n",
    "            Class labels for samples in X.\n",
    "        \"\"\"\n",
    "        decision = self.decision_function(X)\n",
    "        return np.where(decision < 0, self.classes_[0], self.classes_[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel SVM Interactive Visualization\n",
      "====================================\n",
      "Select parameters to visualize Kernel SVM with different datasets and kernels.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070cdb122fc64fd58ea9dec1e92f4a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Dataset:', layout=Layout(width='auto'), options=(('Moons', 'moons'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2020ff0149a8473ea50ba85a7fc34af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_datasets(dataset_type='moons', n_samples=200, noise=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic datasets for classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_type : str, default='moons'\n",
    "        Type of dataset. Can be 'moons', 'circles', or 'linearly_separable'.\n",
    "    n_samples : int, default=200\n",
    "        Number of samples to generate.\n",
    "    noise : float, default=0.2\n",
    "        Standard deviation of Gaussian noise added to the data.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : array-like of shape (n_samples, 2)\n",
    "        Generated features.\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Generated target values.\n",
    "    \"\"\"\n",
    "    if dataset_type == 'moons':\n",
    "        X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "    elif dataset_type == 'circles':\n",
    "        X, y = make_circles(n_samples=n_samples, noise=noise, factor=0.5, random_state=random_state)\n",
    "    elif dataset_type == 'linearly_separable':\n",
    "        X, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0, \n",
    "                                  n_informative=2, random_state=random_state, \n",
    "                                  n_clusters_per_class=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Function to visualize decision boundaries and feature space\n",
    "def visualize_kernel_svm(dataset_type='moons', kernel='rbf', C=1.0, gamma=1.0, \n",
    "                         degree=3, coef0=1.0, n_samples=200, noise=0.2, \n",
    "                         random_state=42, elev=30, azim=30):\n",
    "    \"\"\"\n",
    "    Visualize Kernel SVM decision boundaries and feature space.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_type : str, default='moons'\n",
    "        Type of dataset. Can be 'moons', 'circles', or 'linearly_separable'.\n",
    "    kernel : str, default='rbf'\n",
    "        Kernel type. Can be 'linear', 'polynomial', or 'rbf'.\n",
    "    C : float, default=1.0\n",
    "        Regularization parameter.\n",
    "    gamma : float, default=1.0\n",
    "        Kernel coefficient for 'rbf' and 'polynomial' kernels.\n",
    "    degree : int, default=3\n",
    "        Degree of the polynomial kernel.\n",
    "    coef0 : float, default=1.0\n",
    "        Independent term in the polynomial kernel.\n",
    "    n_samples : int, default=200\n",
    "        Number of samples to generate.\n",
    "    noise : float, default=0.2\n",
    "        Standard deviation of Gaussian noise added to the data.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "    elev : float, default=30\n",
    "        Elevation angle for 3D plot.\n",
    "    azim : float, default=30\n",
    "        Azimuth angle for 3D plot.\n",
    "    \"\"\"\n",
    "    # Generate dataset\n",
    "    X, y = generate_datasets(dataset_type, n_samples, noise, random_state)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Train Kernel SVM\n",
    "    svm = KernelSVM(kernel=kernel, C=C, gamma=gamma, degree=degree, coef0=coef0)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = svm.predict(X_train)\n",
    "    y_test_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Create figure with 2 subplots\n",
    "    fig = plt.figure(figsize=(18, 8))\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Predict on the mesh grid\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax1.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)\n",
    "    \n",
    "    # Plot training and testing data\n",
    "    scatter_train = ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                               edgecolors='k', cmap=plt.cm.RdBu, marker='o', s=80, alpha=0.8)\n",
    "    scatter_test = ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "                              edgecolors='k', cmap=plt.cm.RdBu, marker='^', s=80, alpha=0.8)\n",
    "    \n",
    "    # Plot support vectors\n",
    "    ax1.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], \n",
    "               s=120, facecolors='none', edgecolors='k', linewidths=1.5)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax1.set_xlabel('Feature 1', fontsize=14)\n",
    "    ax1.set_ylabel('Feature 2', fontsize=14)\n",
    "    ax1.set_title(f'Decision Boundary - {dataset_type.capitalize()} Dataset\\n'\n",
    "                 f'{kernel.capitalize()} Kernel (C={C}, γ={gamma})\\n'\n",
    "                 f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}', \n",
    "                 fontsize=16)\n",
    "    ax1.legend([scatter_train, scatter_test], ['Training Data', 'Testing Data'], \n",
    "              loc='upper right', fontsize=12)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot feature space in 3D\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    \n",
    "    # Use Kernel PCA to visualize the feature space\n",
    "    if kernel == 'linear':\n",
    "        # For linear kernel, we can just use PCA\n",
    "        kpca = KernelPCA(n_components=3, kernel='linear')\n",
    "    elif kernel == 'polynomial':\n",
    "        kpca = KernelPCA(n_components=3, kernel='poly', \n",
    "                         gamma=gamma, degree=degree, coef0=coef0)\n",
    "    elif kernel == 'rbf':\n",
    "        kpca = KernelPCA(n_components=3, kernel='rbf', gamma=gamma)\n",
    "    \n",
    "    # Transform data to feature space\n",
    "    X_kpca = kpca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot data in feature space\n",
    "    scatter_feature = ax2.scatter(X_kpca[:, 0], X_kpca[:, 1], X_kpca[:, 2], \n",
    "                                 c=y, edgecolors='k', cmap=plt.cm.RdBu, s=80, alpha=0.8)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax2.set_xlabel('Component 1', fontsize=14)\n",
    "    ax2.set_ylabel('Component 2', fontsize=14)\n",
    "    ax2.set_zlabel('Component 3', fontsize=14)\n",
    "    ax2.set_title(f'Feature Space Projection - {dataset_type.capitalize()} Dataset\\n'\n",
    "                 f'{kernel.capitalize()} Kernel (γ={gamma}' + \n",
    "                 (f', degree={degree}' if kernel == 'polynomial' else '') + ')', \n",
    "                 fontsize=16)\n",
    "    \n",
    "    # Set 3D view angle\n",
    "    ax2.view_init(elev=elev, azim=azim)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return svm, train_accuracy, test_accuracy\n",
    "\n",
    "# Interactive visualization function\n",
    "def interactive_kernel_svm():\n",
    "    \"\"\"\n",
    "    Interactive visualization of Kernel SVM with different datasets and parameters.\n",
    "    \"\"\"\n",
    "    style_widget = {'description_width': 'initial'}\n",
    "    layout_widget = {'width': 'auto'}\n",
    "    \n",
    "    # Create output widget to control display\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Define the update function\n",
    "    def update(dataset_type, kernel, C, gamma, degree, coef0, n_samples, noise, elev, azim):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            visualize_kernel_svm(\n",
    "                dataset_type=dataset_type, \n",
    "                kernel=kernel, \n",
    "                C=C, \n",
    "                gamma=gamma, \n",
    "                degree=degree, \n",
    "                coef0=coef0, \n",
    "                n_samples=n_samples, \n",
    "                noise=noise,\n",
    "                elev=elev,\n",
    "                azim=azim\n",
    "            )\n",
    "    \n",
    "    # Create interactive widgets\n",
    "    dataset_widget = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('Moons', 'moons'), \n",
    "            ('Circles', 'circles'), \n",
    "            ('Linearly Separable', 'linearly_separable')\n",
    "        ],\n",
    "        value='moons',\n",
    "        description='Dataset:',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    kernel_widget = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('Linear Kernel', 'linear'), \n",
    "            ('Polynomial Kernel', 'polynomial'), \n",
    "            ('RBF Kernel', 'rbf')\n",
    "        ],\n",
    "        value='rbf',\n",
    "        description='Kernel:',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    C_widget = widgets.FloatLogSlider(\n",
    "        value=1.0,\n",
    "        base=10,\n",
    "        min=-2,  # 10^-2\n",
    "        max=2,   # 10^2\n",
    "        step=0.1,\n",
    "        description='C (Regularization):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    gamma_widget = widgets.FloatLogSlider(\n",
    "        value=1.0,\n",
    "        base=10,\n",
    "        min=-2,  # 10^-2\n",
    "        max=1,   # 10^1\n",
    "        step=0.1,\n",
    "        description='γ (Scale Parameter):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    degree_widget = widgets.IntSlider(\n",
    "        value=3,\n",
    "        min=1,\n",
    "        max=5,\n",
    "        step=1,\n",
    "        description='Degree (Polynomial):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    coef0_widget = widgets.FloatSlider(\n",
    "        value=1.0,\n",
    "        min=0.0,\n",
    "        max=5.0,\n",
    "        step=0.1,\n",
    "        description='Coef0 (Polynomial):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    n_samples_widget = widgets.IntSlider(\n",
    "        value=200,\n",
    "        min=50,\n",
    "        max=500,\n",
    "        step=50,\n",
    "        description='Number of Samples:',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    noise_widget = widgets.FloatSlider(\n",
    "        value=0.2,\n",
    "        min=0.0,\n",
    "        max=0.5,\n",
    "        step=0.05,\n",
    "        description='Noise Level:',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    elev_widget = widgets.IntSlider(\n",
    "        value=30,\n",
    "        min=0,\n",
    "        max=90,\n",
    "        step=5,\n",
    "        description='Elevation (3D):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    azim_widget = widgets.IntSlider(\n",
    "        value=30,\n",
    "        min=0,\n",
    "        max=360,\n",
    "        step=5,\n",
    "        description='Azimuth (3D):',\n",
    "        style=style_widget,\n",
    "        layout=layout_widget\n",
    "    )\n",
    "    \n",
    "    # Create interactive widget\n",
    "    interactive_widget = widgets.interactive(\n",
    "        update,\n",
    "        dataset_type=dataset_widget,\n",
    "        kernel=kernel_widget,\n",
    "        C=C_widget,\n",
    "        gamma=gamma_widget,\n",
    "        degree=degree_widget,\n",
    "        coef0=coef0_widget,\n",
    "        n_samples=n_samples_widget,\n",
    "        noise=noise_widget,\n",
    "        elev=elev_widget,\n",
    "        azim=azim_widget\n",
    "    )\n",
    "    \n",
    "    # Display the widgets and output\n",
    "    display(interactive_widget)\n",
    "    display(output)\n",
    "    \n",
    "    # Initial update\n",
    "    update(\n",
    "        dataset_type=dataset_widget.value, \n",
    "        kernel=kernel_widget.value, \n",
    "        C=C_widget.value, \n",
    "        gamma=gamma_widget.value, \n",
    "        degree=degree_widget.value, \n",
    "        coef0=coef0_widget.value, \n",
    "        n_samples=n_samples_widget.value, \n",
    "        noise=noise_widget.value,\n",
    "        elev=elev_widget.value,\n",
    "        azim=azim_widget.value\n",
    "    )\n",
    "\n",
    "# Run the interactive visualization\n",
    "print(\"Kernel SVM Interactive Visualization\")\n",
    "print(\"====================================\")\n",
    "print(\"Select parameters to visualize Kernel SVM with different datasets and kernels.\")\n",
    "interactive_kernel_svm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
